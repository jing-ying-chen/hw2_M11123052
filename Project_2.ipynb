{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from itertools import chain\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.morphology import label\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f511ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.bool)\n",
    "    y_pred = y_pred.astype(np.bool)\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    union = np.logical_or(y_true, y_pred)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928111d",
   "metadata": {},
   "source": [
    "# 影像前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (256, 256)\n",
    "learning_rate = 0.0001\n",
    "batch_size = 2\n",
    "epochs = 40\n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13188ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 訓練資料_Image\n",
    "# 設定文件夾路徑和圖像大小\n",
    "X_train_path = \"C://ETT_v3/Fold1/train/\"\n",
    "y_train_path = \"C://ETT_v3/Fold1/trainannot/\"\n",
    "\n",
    "# 取得文件夾中所有的圖像檔名\n",
    "X_train_names = os.listdir(X_train_path)\n",
    "y_train_names = os.listdir(y_train_path)\n",
    "\n",
    "# 初始化空的numpy陣列以儲存圖像\n",
    "X_train = np.zeros((len(X_train_names)*3, img_size[0], img_size[1], 1))\n",
    "y_train = np.zeros((len(y_train_names)*3, img_size[0], img_size[1], 1))\n",
    "\n",
    "# 遍歷每個圖檔，使用Load_img()讀取圖檔\n",
    "for i, X_name in enumerate(X_train_names):\n",
    "    img = load_img(X_train_path + X_name, target_size=img_size)\n",
    "    img_arr = np.array(img)\n",
    "    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n",
    "    X_train[i] = img_gray.reshape((256, 256, 1))\n",
    "    \n",
    "    # 對比度處理 \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_clahe = clahe.apply(img_gray)\n",
    "    img_clahe_reshaped = img_clahe.reshape((256, 256, 1))\n",
    "    X_train[i+len(X_train_names)] = img_clahe_reshaped\n",
    "    \n",
    "    # 水平翻轉處理\n",
    "    img_flipped = cv2.flip(img_clahe_reshaped, 1).reshape((256, 256, 1))\n",
    "    X_train[i+len(X_train_names)*2] = img_flipped\n",
    "\n",
    "print(\"Train image data shape:\", X_train.shape)\n",
    "\n",
    "# 訓練資料_mask\n",
    "for i, y_name in enumerate(y_train_names):\n",
    "    img = load_img(y_train_path + y_name, target_size=img_size)\n",
    "    img_arr = np.array(img)\n",
    "    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY).reshape((256, 256, 1))\n",
    "    y_train[i] = img_gray\n",
    "    y_train[i+len(X_train_names)] = img_gray\n",
    "    \n",
    "    # 水平翻轉處理\n",
    "    img_flipped = cv2.flip(img_gray, 1).reshape((256, 256, 1))\n",
    "    y_train[i+len(X_train_names)*2] = img_flipped\n",
    "    \n",
    "# 二值化\n",
    "y_train = np.where(y_train > 127, 1, 0)\n",
    "\n",
    "print(\"Train mask data shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1019c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 顯示原始圖像、對比強化後和水平翻轉後的圖像(第一張為例)\n",
    "index = 0\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(X_train[index].astype('uint8'), cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(X_train[index+len(X_train_names)].astype('uint8'), cmap='gray')\n",
    "plt.title('CLAHE Image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(X_train[index+len(X_train_names)*2].astype('uint8'), cmap='gray')\n",
    "plt.title('Flipped Image')\n",
    "plt.show()\n",
    "\n",
    "# 顯示原始mask、水平翻轉後的mask(第一張為例)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(y_train[index].astype('uint8'), cmap='gray')\n",
    "plt.title('Original Mask')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(y_train[index+len(y_train_names)*2].astype('uint8'), cmap='gray')\n",
    "plt.title('Flipped Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82bd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 驗證資料\n",
    "X_val_path = \"C://ETT_v3/Fold1/val/\"\n",
    "y_val_path = \"C://ETT_v3/Fold1/valannot/\"\n",
    "\n",
    "X_val_names = os.listdir(X_val_path)\n",
    "y_val_names = os.listdir(y_val_path)\n",
    "\n",
    "X_val = np.zeros((len(X_val_names), img_size[0], img_size[1], 1))\n",
    "y_val = np.zeros((len(y_val_names), img_size[0], img_size[1], 1))\n",
    "\n",
    "for i, X_name in enumerate(X_val_names):\n",
    "    img = load_img(X_val_path + X_name, target_size=img_size)\n",
    "    img_arr = np.array(img)\n",
    "    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n",
    "    X_val[i] = img_gray.reshape((256, 256, 1))\n",
    "\n",
    "print(\"Val image data shape:\", X_val.shape)\n",
    "\n",
    "for i, y_name in enumerate(y_val_names):\n",
    "    img = load_img(y_val_path + y_name, target_size=img_size)\n",
    "    img_arr = np.array(img)\n",
    "    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n",
    "    y_val[i] = img_gray.reshape((256, 256, 1))\n",
    "    \n",
    "y_val = np.where(y_val > 127, 1, 0)\n",
    "\n",
    "print(\"Val mask data shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e51d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試資料\n",
    "X_test_path = \"C://ETT_v3/Fold1/test/\"\n",
    "y_test_path = \"C://ETT_v3/Fold1/testannot/\"\n",
    "\n",
    "X_test_names = os.listdir(X_test_path)\n",
    "y_test_names = os.listdir(y_test_path)\n",
    "\n",
    "X_test = np.zeros((len(X_test_names), img_size[0], img_size[1], 1))\n",
    "y_test = np.zeros((len(y_test_names), img_size[0], img_size[1], 1))\n",
    "original = {}\n",
    "\n",
    "for i, X_name in enumerate(X_test_names):\n",
    "    img = load_img(X_test_path + X_name, target_size=img_size)\n",
    "    img_arr = np.array(img)\n",
    "    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n",
    "    X_test[i] = img_gray.reshape((256, 256, 1))\n",
    "\n",
    "print(\"Test image data shape:\", X_test.shape)\n",
    "\n",
    "for i, y_name in enumerate(y_test_names):\n",
    "    # 取得遮罩原圖尺寸\n",
    "    original_img = Image.open(os.path.join(y_test_path, y_name))\n",
    "    original_img_arr = np.array(original_img)\n",
    "    original_img_gary = cv2.cvtColor(original_img_arr, cv2.COLOR_BGR2GRAY)\n",
    "    original[i] = {'img': original_img_gary, 'img_size': original_img.size}\n",
    "    \n",
    "    img = load_img(y_test_path + y_name, target_size=img_size)\n",
    "    img_arr = np.array(img)\n",
    "    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n",
    "    y_test[i] = img_gray.reshape((256, 256, 1))\n",
    "    \n",
    "y_test = np.where(y_test > 127, 1, 0)\n",
    "\n",
    "print(\"Test mask data shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7f94c",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef07e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Unet(input_img, n_filters = 2, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30289161",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "input_img = Input((256, 256, 1), name='img')\n",
    "model = Unet(input_img, n_filters=1, dropout=0.05, batchnorm=True)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"binary_crossentropy\", metrics=tf.keras.metrics.AUC())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d80680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model-Unet.h5', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    CSVLogger(\"Unet.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964e99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d67edb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"auc\"], label=\"Accuracy\")\n",
    "plt.plot(results.history[\"val_auc\"], label=\"val_Accuracy\")\n",
    "plt.plot(np.argmax(results.history[\"val_auc\"]), np.max(results.history[\"val_auc\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2547e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-Unet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2dcb13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#二值化\n",
    "print(np.unique(preds))\n",
    "\n",
    "preds = np.where(preds >= 0.23289567, 1, 0)\n",
    "\n",
    "print(np.unique(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e0cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 顯示原始圖像、原始mask和預測mask(第一張為例)\n",
    "index = 0\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(y_test[index], cmap='gray')\n",
    "plt.title('Ground Truth Mask')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(preds[index], cmap='gray')\n",
    "plt.title('Predicted Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算預測結果的IOU\n",
    "iou_score = iou(y_test, preds)\n",
    "\n",
    "print('IOU: ', iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77dfb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 計算所有圖像中最下面端點的誤差（以像素為單位）\n",
    "total_error = 0\n",
    "num_error_half = 0\n",
    "num_error_one = 0\n",
    "num_images = len(y_test)\n",
    "\n",
    "for i in range(num_images):\n",
    "    \n",
    "    # 找到原始遮罩中的端點\n",
    "    binary_img = cv2.convertScaleAbs(original[i]['img'])\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    bottom_point_gt = tuple(max_contour[max_contour[:,:,1].argmax()][0])\n",
    "    \n",
    "    # 找到預測遮罩中的端點\n",
    "    pred_mask_uint8 = preds[i].astype(np.uint8) # 將預測的遮罩轉換為8位無符號整數類型\n",
    "    resized_pred_mask = cv2.resize(pred_mask_uint8, original[i]['img_size']) # 縮放預測的遮罩為原始尺寸\n",
    "    binary_img = cv2.convertScaleAbs(resized_pred_mask) # 將縮放後的遮罩轉為二值圖像\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    bottom_point_pred  = tuple(max_contour[max_contour[:,:,1].argmax()][0])\n",
    "\n",
    "    # 計算端點的誤差（以像素為單位）\n",
    "    error_pixels = np.abs(bottom_point_gt[1] - bottom_point_pred[1])\n",
    "    \n",
    "    # 將誤差從像素轉換為公分\n",
    "    error_cm = error_pixels / 72\n",
    "    \n",
    "    # 加入總誤差中\n",
    "    total_error += error_cm\n",
    "    \n",
    "    # 如果誤差<=0.5公分、<=1公分，則將符合條件的樣本數加1\n",
    "    if error_cm <= 0.5:\n",
    "        num_error_half += 1\n",
    "    \n",
    "    if error_cm <= 1:\n",
    "        num_error_one += 1\n",
    "\n",
    "# 計算平均誤差（以公分為單位）\n",
    "mean_error_cm = total_error / num_images\n",
    "\n",
    "# 計算誤差<=0.5公分、<=1公分的準確率\n",
    "accuracy_half = num_error_half / num_images\n",
    "accuracy_one = num_error_one / num_images\n",
    "\n",
    "print('平均誤差（公分）:', mean_error_cm)\n",
    "print('誤差小於等於0.5公分的準確率:', accuracy_half*100, '%')\n",
    "print('誤差小於等於1公分的準確率:', accuracy_one*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa09c2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 顯示預測遮罩的輪廓和端點(以第一張為例)\n",
    "index = 0\n",
    "\n",
    "# 將預測圖像轉換為CV_8UC1格式\n",
    "pred_mask_uint8 = preds[index].astype(np.uint8) *255\n",
    "\n",
    "# 縮放預測的遮罩為原始尺寸\n",
    "resized_pred_mask = cv2.resize(pred_mask_uint8, original[i]['img_size'])\n",
    "\n",
    "# 將縮放後的遮罩轉為二值圖像\n",
    "binary_img = cv2.convertScaleAbs(resized_pred_mask)\n",
    "\n",
    "# 執行輪廓檢測\n",
    "contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 找到最大的輪廓\n",
    "max_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "# 找到最下面的點\n",
    "bottom_point_pred = tuple(max_contour[max_contour[:,:,1].argmax()][0])\n",
    "\n",
    "# 輸出端點\n",
    "print(\"Bottom point:\", bottom_point_pred)\n",
    "\n",
    "# 创建彩色图像\n",
    "contour_image = cv2.cvtColor(binary_img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# 绘制轮廓\n",
    "cv2.drawContours(contour_image, [max_contour], -1, (0, 255, 0), 25)\n",
    "\n",
    "# 绘制端点\n",
    "cv2.circle(contour_image, bottom_point_pred, 50, (255, 0, 0), -1)\n",
    "\n",
    "# 显示图像\n",
    "plt.imshow(contour_image)\n",
    "plt.title('Mask with Contours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846d8d1",
   "metadata": {},
   "source": [
    "# U-Net++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d645e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "\n",
    "def conv_batchnorm_relu_block(input_tensor, nb_filter, kernel_size=3):\n",
    "\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), padding='same')(input_tensor)\n",
    "    x = BatchNormalization(axis=2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def UnetPP(input_shape, n_labels, using_deep_supervision=False):\n",
    "\n",
    "    nb_filter = [32,64,128,256,512]\n",
    "\n",
    "    # Set image data format to channels first\n",
    "    global bn_axis\n",
    "\n",
    "    K.set_image_data_format(\"channels_last\")\n",
    "    bn_axis = -1\n",
    "    inputs = Input(shape=input_shape, name='input_image')\n",
    "\n",
    "    conv1_1 = conv_batchnorm_relu_block(inputs, nb_filter=nb_filter[0])\n",
    "    pool1 = AvgPool2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
    "\n",
    "    conv2_1 = conv_batchnorm_relu_block(pool1, nb_filter=nb_filter[1])\n",
    "    pool2 = AvgPool2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
    "\n",
    "    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
    "    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
    "    conv1_2 = conv_batchnorm_relu_block(conv1_2,  nb_filter=nb_filter[0])\n",
    "\n",
    "    conv3_1 = conv_batchnorm_relu_block(pool2, nb_filter=nb_filter[2])\n",
    "    pool3 = AvgPool2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
    "\n",
    "    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
    "    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
    "    conv2_2 = conv_batchnorm_relu_block(conv2_2, nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
    "    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
    "    conv1_3 = conv_batchnorm_relu_block(conv1_3, nb_filter=nb_filter[0])\n",
    "\n",
    "    conv4_1 = conv_batchnorm_relu_block(pool3, nb_filter=nb_filter[3])\n",
    "    pool4 = AvgPool2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
    "\n",
    "    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
    "    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
    "    conv3_2 = conv_batchnorm_relu_block(conv3_2, nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
    "    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
    "    conv2_3 = conv_batchnorm_relu_block(conv2_3, nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
    "    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
    "    conv1_4 = conv_batchnorm_relu_block(conv1_4, nb_filter=nb_filter[0])\n",
    "\n",
    "    conv5_1 = conv_batchnorm_relu_block(pool4, nb_filter=nb_filter[4])\n",
    "\n",
    "    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
    "    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
    "    conv4_2 = conv_batchnorm_relu_block(conv4_2, nb_filter=nb_filter[3])\n",
    "\n",
    "    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
    "    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
    "    conv3_3 = conv_batchnorm_relu_block(conv3_3, nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
    "    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
    "    conv2_4 = conv_batchnorm_relu_block(conv2_4, nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
    "    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
    "    conv1_5 = conv_batchnorm_relu_block(conv1_5, nb_filter=nb_filter[0])\n",
    "\n",
    "    nestnet_output_1 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_1',padding='same')(conv1_2)\n",
    "    nestnet_output_2 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_2', padding='same' )(conv1_3)\n",
    "    nestnet_output_3 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_3', padding='same')(conv1_4)\n",
    "    nestnet_output_4 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_4', padding='same')(conv1_5)\n",
    "\n",
    "    if using_deep_supervision:\n",
    "        model = Model(input=inputs, output=[nestnet_output_1,\n",
    "                                            nestnet_output_2,\n",
    "                                            nestnet_output_3,\n",
    "                                            nestnet_output_4])\n",
    "    else:\n",
    "        model = Model(inputs=inputs, outputs=nestnet_output_4)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223614ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = UnetPP(input_shape = (256, 256, 1), n_labels=1)\n",
    "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=tf.keras.metrics.AUC())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model-UnetPP.h5', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    CSVLogger(\"dataUnetPP.csv\"),\n",
    "    TensorBoard(log_dir='./logs')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a64f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4730d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"auc\"], label=\"Accuracy\")\n",
    "plt.plot(results.history[\"val_auc\"], label=\"val_Accuracy\")\n",
    "plt.plot(np.argmax(results.history[\"val_auc\"]), np.max(results.history[\"val_auc\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-UnetPP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a070e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44eb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#二值化\n",
    "print(np.unique(preds))\n",
    "\n",
    "preds = np.where(preds >= 0.8, 1, 0)\n",
    "\n",
    "print(np.unique(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861440bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示原始圖像、原始mask和預測mask(第一張為例)\n",
    "index = 0\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(X_test[index], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(y_test[index], cmap='gray')\n",
    "plt.title('Ground Truth Mask')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(preds[index], cmap='gray')\n",
    "plt.title('Predicted Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8382cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算預測結果的IOU\n",
    "iou_score = iou(y_test, preds)\n",
    "\n",
    "print('IOU: ', iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算所有圖像中最下面端點的誤差（以像素為單位）\n",
    "total_error = 0\n",
    "num_error_half = 0\n",
    "num_error_one = 0\n",
    "num_images = len(y_test)\n",
    "\n",
    "for i in range(num_images):\n",
    "    \n",
    "    # 找到原始遮罩中的端點\n",
    "    binary_img = cv2.convertScaleAbs(original[i]['img'])\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    bottom_point_gt = tuple(max_contour[max_contour[:,:,1].argmax()][0])\n",
    "    \n",
    "    # 找到預測遮罩中的端點\n",
    "    pred_mask_uint8 = preds[i].astype(np.uint8) # 將預測的遮罩轉換為8位無符號整數類型\n",
    "    resized_pred_mask = cv2.resize(pred_mask_uint8, original[i]['img_size']) # 縮放預測的遮罩為原始尺寸\n",
    "    binary_img = cv2.convertScaleAbs(resized_pred_mask) # 將縮放後的遮罩轉為二值圖像\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    bottom_point_pred  = tuple(max_contour[max_contour[:,:,1].argmax()][0])\n",
    "\n",
    "    # 計算端點的誤差（以像素為單位）\n",
    "    error_pixels = np.abs(bottom_point_gt[1] - bottom_point_pred[1])\n",
    "    \n",
    "    # 將誤差從像素轉換為公分\n",
    "    error_cm = error_pixels / 72\n",
    "    \n",
    "    # 加入總誤差中\n",
    "    total_error += error_cm\n",
    "    \n",
    "    # 如果誤差<=0.5公分、<=1公分，則將符合條件的樣本數加1\n",
    "    if error_cm <= 0.5:\n",
    "        num_error_half += 1\n",
    "    \n",
    "    if error_cm <= 1:\n",
    "        num_error_one += 1\n",
    "\n",
    "# 計算平均誤差（以公分為單位）\n",
    "mean_error_cm = total_error / num_images\n",
    "\n",
    "# 計算誤差<=0.5公分、<=1公分的準確率\n",
    "accuracy_half = num_error_half / num_images\n",
    "accuracy_one = num_error_one / num_images\n",
    "\n",
    "print('平均誤差（公分）:', mean_error_cm)\n",
    "print('誤差小於等於0.5公分的準確率:', accuracy_half*100, '%')\n",
    "print('誤差小於等於1公分的準確率:', accuracy_one*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ec13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示預測遮罩的輪廓和端點(以第一張為例)\n",
    "index = 0\n",
    "\n",
    "# 將預測圖像轉換為CV_8UC1格式\n",
    "pred_mask_uint8 = preds[index].astype(np.uint8) *255\n",
    "\n",
    "# 縮放預測的遮罩為原始尺寸\n",
    "resized_pred_mask = cv2.resize(pred_mask_uint8, original[i]['img_size'])\n",
    "\n",
    "# 將縮放後的遮罩轉為二值圖像\n",
    "binary_img = cv2.convertScaleAbs(resized_pred_mask)\n",
    "\n",
    "# 執行輪廓檢測\n",
    "contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 找到最大的輪廓\n",
    "max_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "# 找到最下面的點\n",
    "bottom_point_pred = tuple(max_contour[max_contour[:,:,1].argmax()][0])\n",
    "\n",
    "# 輸出端點\n",
    "print(\"Bottom point:\", bottom_point_pred)\n",
    "\n",
    "# 创建彩色图像\n",
    "contour_image = cv2.cvtColor(binary_img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# 绘制轮廓\n",
    "cv2.drawContours(contour_image, [max_contour], -1, (0, 255, 0), 25)\n",
    "\n",
    "# 绘制端点\n",
    "cv2.circle(contour_image, bottom_point_pred, 50, (255, 0, 0), -1)\n",
    "\n",
    "# 显示图像\n",
    "plt.imshow(contour_image)\n",
    "plt.title('Mask with Contours')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "another_name",
   "language": "python",
   "name": "another_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
